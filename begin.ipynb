{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596478105988",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINNs: a start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import scipy.io\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from net import Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navier-Stokes equation\n",
    "\n",
    "Given a two-dimensional velocity field, \n",
    "\n",
    "$$ {\\bf V}(x, y; t)  =  ( u(x, y; t), v(x, y; t) )  \\; \\;,  $$\n",
    "its components satisfy the following equations\n",
    "\n",
    "$$ u_t + \\lambda_1 ( u u_x + v u_y ) = -p_x + \\lambda_2 ( u_{xx} + u_{yy}) $$\n",
    "$$ v_t + \\lambda_1 ( u v_x + v v_y ) = -p_y+ \\lambda_2 ( v_{xx} + v_{yy}) $$\n",
    "with $ p = p(x, y; t)$ being the pressure. The parameters $\\lambda_1 $ and $\\lambda_2$ are unknown.\n",
    "\n",
    "We are interested in learning the parameters $\\{ \\lambda \\} $ as well as the pressure $ p(x, y; t)$.\n",
    "\n",
    "Solutions are searched in a set satisfying continuity equation $ \\nabla \\cdot {\\bf V}(x, y; t) = 0 $, \n",
    "\n",
    "$$ u_x + v_y = 0  \\; \\;.$$\n",
    "\n",
    "Defining a latent function $ \\psi = \\psi(x, y; t) $ such that (how crucial is the use of $\\psi$?):\n",
    "$$ u = \\psi_y   \\;, $$ \n",
    "$$ v = - \\psi_x  \\;, $$ \n",
    "the continuity equation is satistied. Given a set ${\\cal S}$ of (noisy) measurements of the velocity field, \n",
    "\n",
    "$$    {\\cal S} = \\{ t^{(j)}, x^{(j)}, y^{(j)} , u^{(j)}, v^{(j)}   \\}_{j=1}^{N}  \\;, $$\n",
    "we define\n",
    "\n",
    "$$ f(x, y; t) \\equiv u_t + \\lambda_1 ( u u_x + v u_y )  + p_x - \\lambda_2 ( u_{xx} + u_{yy}) \\;,$$    \n",
    "$$  g(x, y; t) \\equiv v_t + \\lambda_1 ( u v_x + v v_y ) + p_y - \\lambda_2 ( v_{xx} + v_{yy})  \\;,$$\n",
    "and proceed by jointly approximating\n",
    "\n",
    "$$ \\left[   \\psi(x, y; t) ;  p(x, y; t)  \\right]  \\;, $$\n",
    "using a single neural network with two outputs.\n",
    "\n",
    "The prior assumption is taking into account in another neural network (PINN) with two outputs:\n",
    "\n",
    "$$ \\left[   f(x, y; t) ;  g(x, y; t)  \\right]  \\;. $$\n",
    "\n",
    "The parameters $\\{ \\lambda \\} $ operate as well as the parameters of the neural networks:\n",
    "\n",
    "\n",
    "$$ \\left[   \\psi(x, y; t) ;  p(x, y; t)  \\right]  \\;, $$\n",
    "$$ \\left[   f(x, y; t) ;  g(x, y; t)  \\right]  \\;, $$\n",
    "that can be trained using a mean squared error loss:\n",
    "\n",
    "$$  MSE \\equiv \\frac{1}{N} \\sum_{j=1}^{N} \\left[  \\left( u( x^{(j)}, y^{(j)}, t^{(j)}) - u^{(j)} \\right)^2 + \\left( v( x^{(j)}, y^{(j)}, t^{(j)}) - v^{(j)} \\right)^2  \\right]  + \\frac{1}{N} \\sum_{j=1}^{N} \\left[   |  u( x^{(j)}, y^{(j)}, t^{(j)}) |^2 +  |g( x^{(j)}, y^{(j)}, t^{(j)})|^2       \\right]     $$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "From https://github.com/maziarraissi/PINNs/blob/master/main/Data/cylinder_nektar_wake.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('data/cylinder_nektar_wake.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'__header__': b'MATLAB 5.0 MAT-file Platform: posix, Created on: Fri Sep 22 23:02:15 2017',\n '__version__': '1.0',\n '__globals__': [],\n 'X_star': array([[ 1.        , -2.        ],\n        [ 1.07070707, -2.        ],\n        [ 1.14141414, -2.        ],\n        ...,\n        [ 7.85858586,  2.        ],\n        [ 7.92929293,  2.        ],\n        [ 8.        ,  2.        ]]),\n 't': array([[ 0. ],\n        [ 0.1],\n        [ 0.2],\n        [ 0.3],\n        [ 0.4],\n        [ 0.5],\n        [ 0.6],\n        [ 0.7],\n        [ 0.8],\n        [ 0.9],\n        [ 1. ],\n        [ 1.1],\n        [ 1.2],\n        [ 1.3],\n        [ 1.4],\n        [ 1.5],\n        [ 1.6],\n        [ 1.7],\n        [ 1.8],\n        [ 1.9],\n        [ 2. ],\n        [ 2.1],\n        [ 2.2],\n        [ 2.3],\n        [ 2.4],\n        [ 2.5],\n        [ 2.6],\n        [ 2.7],\n        [ 2.8],\n        [ 2.9],\n        [ 3. ],\n        [ 3.1],\n        [ 3.2],\n        [ 3.3],\n        [ 3.4],\n        [ 3.5],\n        [ 3.6],\n        [ 3.7],\n        [ 3.8],\n        [ 3.9],\n        [ 4. ],\n        [ 4.1],\n        [ 4.2],\n        [ 4.3],\n        [ 4.4],\n        [ 4.5],\n        [ 4.6],\n        [ 4.7],\n        [ 4.8],\n        [ 4.9],\n        [ 5. ],\n        [ 5.1],\n        [ 5.2],\n        [ 5.3],\n        [ 5.4],\n        [ 5.5],\n        [ 5.6],\n        [ 5.7],\n        [ 5.8],\n        [ 5.9],\n        [ 6. ],\n        [ 6.1],\n        [ 6.2],\n        [ 6.3],\n        [ 6.4],\n        [ 6.5],\n        [ 6.6],\n        [ 6.7],\n        [ 6.8],\n        [ 6.9],\n        [ 7. ],\n        [ 7.1],\n        [ 7.2],\n        [ 7.3],\n        [ 7.4],\n        [ 7.5],\n        [ 7.6],\n        [ 7.7],\n        [ 7.8],\n        [ 7.9],\n        [ 8. ],\n        [ 8.1],\n        [ 8.2],\n        [ 8.3],\n        [ 8.4],\n        [ 8.5],\n        [ 8.6],\n        [ 8.7],\n        [ 8.8],\n        [ 8.9],\n        [ 9. ],\n        [ 9.1],\n        [ 9.2],\n        [ 9.3],\n        [ 9.4],\n        [ 9.5],\n        [ 9.6],\n        [ 9.7],\n        [ 9.8],\n        [ 9.9],\n        [10. ],\n        [10.1],\n        [10.2],\n        [10.3],\n        [10.4],\n        [10.5],\n        [10.6],\n        [10.7],\n        [10.8],\n        [10.9],\n        [11. ],\n        [11.1],\n        [11.2],\n        [11.3],\n        [11.4],\n        [11.5],\n        [11.6],\n        [11.7],\n        [11.8],\n        [11.9],\n        [12. ],\n        [12.1],\n        [12.2],\n        [12.3],\n        [12.4],\n        [12.5],\n        [12.6],\n        [12.7],\n        [12.8],\n        [12.9],\n        [13. ],\n        [13.1],\n        [13.2],\n        [13.3],\n        [13.4],\n        [13.5],\n        [13.6],\n        [13.7],\n        [13.8],\n        [13.9],\n        [14. ],\n        [14.1],\n        [14.2],\n        [14.3],\n        [14.4],\n        [14.5],\n        [14.6],\n        [14.7],\n        [14.8],\n        [14.9],\n        [15. ],\n        [15.1],\n        [15.2],\n        [15.3],\n        [15.4],\n        [15.5],\n        [15.6],\n        [15.7],\n        [15.8],\n        [15.9],\n        [16. ],\n        [16.1],\n        [16.2],\n        [16.3],\n        [16.4],\n        [16.5],\n        [16.6],\n        [16.7],\n        [16.8],\n        [16.9],\n        [17. ],\n        [17.1],\n        [17.2],\n        [17.3],\n        [17.4],\n        [17.5],\n        [17.6],\n        [17.7],\n        [17.8],\n        [17.9],\n        [18. ],\n        [18.1],\n        [18.2],\n        [18.3],\n        [18.4],\n        [18.5],\n        [18.6],\n        [18.7],\n        [18.8],\n        [18.9],\n        [19. ],\n        [19.1],\n        [19.2],\n        [19.3],\n        [19.4],\n        [19.5],\n        [19.6],\n        [19.7],\n        [19.8],\n        [19.9]]),\n 'U_star': array([[[ 1.11419216e+00,  1.11755721e+00,  1.11956933e+00, ...,\n           1.16105653e+00,  1.16209316e+00,  1.16287446e+00],\n         [-4.09649775e-03,  1.09212754e-03,  3.28231641e-03, ...,\n          -1.14679740e-02, -1.46226631e-02, -1.78649950e-02]],\n \n        [[ 1.11102707e+00,  1.11424311e+00,  1.11623549e+00, ...,\n           1.16119046e+00,  1.16250501e+00,  1.16355997e+00],\n         [ 3.93130751e-04,  6.09231658e-03,  8.54240777e-03, ...,\n          -3.69331211e-03, -6.90960992e-03, -1.02375054e-02]],\n \n        [[ 1.10748452e+00,  1.11049848e+00,  1.11244362e+00, ...,\n           1.16080365e+00,  1.16241577e+00,  1.16375737e+00],\n         [ 4.42777717e-03,  1.06585027e-02,  1.33831464e-02, ...,\n           4.14245483e-03,  8.86993440e-04, -2.50609725e-03]],\n \n        ...,\n \n        [[ 1.04183122e+00,  1.07052197e+00,  1.08570786e+00, ...,\n           1.03561691e+00,  1.01482352e+00,  9.95607444e-01],\n         [-1.53233747e-01, -1.52906494e-01, -1.50934775e-01, ...,\n           1.70410243e-01,  1.74234015e-01,  1.74978509e-01]],\n \n        [[ 1.03144583e+00,  1.05940483e+00,  1.07424671e+00, ...,\n           1.05086925e+00,  1.02921296e+00,  1.00907934e+00],\n         [-1.52570327e-01, -1.53932097e-01, -1.52973975e-01, ...,\n           1.65974219e-01,  1.72307867e-01,  1.75206081e-01]],\n \n        [[ 1.02128975e+00,  1.04848021e+00,  1.06294889e+00, ...,\n           1.06667284e+00,  1.04430400e+00,  1.02333582e+00],\n         [-1.51330184e-01, -1.54252780e-01, -1.54219310e-01, ...,\n           1.59519620e-01,  1.68613705e-01,  1.73914810e-01]]]),\n 'p_star': array([[-0.10815457, -0.11115509, -0.1123679 , ..., -0.10035143,\n         -0.09845376, -0.09658143],\n        [-0.10560371, -0.10881921, -0.1101648 , ..., -0.10137256,\n         -0.09959008, -0.09781616],\n        [-0.10257633, -0.10599348, -0.10746686, ..., -0.10204929,\n         -0.10039933, -0.09874095],\n        ...,\n        [ 0.0066028 ,  0.00272234,  0.00081532, ...,  0.01152894,\n          0.01414437,  0.01651662],\n        [ 0.007892  ,  0.00402109,  0.00209731, ...,  0.00927242,\n          0.01200894,  0.01451151],\n        [ 0.00915729,  0.00530504,  0.00337089, ...,  0.00691424,\n          0.00975565,  0.01238328]])}"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(200, 1)\n(5000, 2)\n(5000, 2, 200)\n(5000, 200)\n"
    }
   ],
   "source": [
    "print(data['t'].shape)\n",
    "print(data['X_star'].shape)\n",
    "print(data['U_star'].shape)\n",
    "print(data['p_star'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_star = data['U_star'] # N x 2 x T\n",
    "p_star = data['p_star'] # N x T\n",
    "t_star = data['t'] # T x 1\n",
    "X_star = data['X_star'] # N x 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[[ 1.11419216e+00,  1.11755721e+00,  1.11956933e+00, ...,\n          1.16105653e+00,  1.16209316e+00,  1.16287446e+00],\n        [-4.09649775e-03,  1.09212754e-03,  3.28231641e-03, ...,\n         -1.14679740e-02, -1.46226631e-02, -1.78649950e-02]],\n\n       [[ 1.11102707e+00,  1.11424311e+00,  1.11623549e+00, ...,\n          1.16119046e+00,  1.16250501e+00,  1.16355997e+00],\n        [ 3.93130751e-04,  6.09231658e-03,  8.54240777e-03, ...,\n         -3.69331211e-03, -6.90960992e-03, -1.02375054e-02]],\n\n       [[ 1.10748452e+00,  1.11049848e+00,  1.11244362e+00, ...,\n          1.16080365e+00,  1.16241577e+00,  1.16375737e+00],\n        [ 4.42777717e-03,  1.06585027e-02,  1.33831464e-02, ...,\n          4.14245483e-03,  8.86993440e-04, -2.50609725e-03]],\n\n       ...,\n\n       [[ 1.04183122e+00,  1.07052197e+00,  1.08570786e+00, ...,\n          1.03561691e+00,  1.01482352e+00,  9.95607444e-01],\n        [-1.53233747e-01, -1.52906494e-01, -1.50934775e-01, ...,\n          1.70410243e-01,  1.74234015e-01,  1.74978509e-01]],\n\n       [[ 1.03144583e+00,  1.05940483e+00,  1.07424671e+00, ...,\n          1.05086925e+00,  1.02921296e+00,  1.00907934e+00],\n        [-1.52570327e-01, -1.53932097e-01, -1.52973975e-01, ...,\n          1.65974219e-01,  1.72307867e-01,  1.75206081e-01]],\n\n       [[ 1.02128975e+00,  1.04848021e+00,  1.06294889e+00, ...,\n          1.06667284e+00,  1.04430400e+00,  1.02333582e+00],\n        [-1.51330184e-01, -1.54252780e-01, -1.54219310e-01, ...,\n          1.59519620e-01,  1.68613705e-01,  1.73914810e-01]]])"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "U_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 1.        , -2.        ],\n       [ 1.07070707, -2.        ],\n       [ 1.14141414, -2.        ],\n       ...,\n       [ 7.85858586,  2.        ],\n       [ 7.92929293,  2.        ],\n       [ 8.        ,  2.        ]])"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "X_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([-2.        , -1.91836735, -1.83673469, -1.75510204, -1.67346939,\n       -1.59183673, -1.51020408, -1.42857143, -1.34693878, -1.26530612,\n       -1.18367347, -1.10204082, -1.02040816, -0.93877551, -0.85714286,\n       -0.7755102 , -0.69387755, -0.6122449 , -0.53061224, -0.44897959,\n       -0.36734694, -0.28571429, -0.20408163, -0.12244898, -0.04081633,\n        0.04081633,  0.12244898,  0.20408163,  0.28571429,  0.36734694,\n        0.44897959,  0.53061224,  0.6122449 ,  0.69387755,  0.7755102 ,\n        0.85714286,  0.93877551,  1.02040816,  1.10204082,  1.18367347,\n        1.26530612,  1.34693878,  1.42857143,  1.51020408,  1.59183673,\n        1.67346939,  1.75510204,  1.83673469,  1.91836735,  2.        ])"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "np.unique(X_star[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(5000, 2)"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "X_star.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[-0.10815457, -0.11115509, -0.1123679 , ..., -0.10035143,\n        -0.09845376, -0.09658143],\n       [-0.10560371, -0.10881921, -0.1101648 , ..., -0.10137256,\n        -0.09959008, -0.09781616],\n       [-0.10257633, -0.10599348, -0.10746686, ..., -0.10204929,\n        -0.10039933, -0.09874095],\n       ...,\n       [ 0.0066028 ,  0.00272234,  0.00081532, ...,  0.01152894,\n         0.01414437,  0.01651662],\n       [ 0.007892  ,  0.00402109,  0.00209731, ...,  0.00927242,\n         0.01200894,  0.01451151],\n       [ 0.00915729,  0.00530504,  0.00337089, ...,  0.00691424,\n         0.00975565,  0.01238328]])"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "p_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X_star.shape[0]\n",
    "T = t_star.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange Data \n",
    "XX = np.tile(X_star[:, 0:1], (1, T)) # N x T\n",
    "YY = np.tile(X_star[:, 1:2], (1, T)) # N x T\n",
    "TT = np.tile(t_star, (1, N)).T # N x T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(5000, 200)\n(5000, 200)\n(5000, 200)\n"
    }
   ],
   "source": [
    "print(XX.shape)\n",
    "print(YY.shape)\n",
    "print(TT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange Data \n",
    "UU = U_star[:, 0, :] # N x T\n",
    "VV = U_star[:, 1, :] # N x T\n",
    "pp = p_star # N x T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(5000, 200)\n(5000, 200)\n(5000, 200)\n"
    }
   ],
   "source": [
    "print(UU.shape)\n",
    "print(VV.shape)\n",
    "print(pp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Flattening\n",
    "x = XX.flatten()[:, None] # NT x 1\n",
    "y = YY.flatten()[:, None] # NT x 1\n",
    "t = TT.flatten()[:, None] # NT x 1\n",
    "    \n",
    "u = UU.flatten()[:, None] # NT x 1\n",
    "v = VV.flatten()[:, None] # NT x 1\n",
    "p = pp.flatten()[:, None] # NT x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1000000, 1)\n(1000000, 1)\n(1000000, 1)\n(1000000, 1)\n(1000000, 1)\n(1000000, 1)\n"
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(t.shape)\n",
    "print(u.shape)\n",
    "print(v.shape)\n",
    "print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data    \n",
    "N_train = 5000\n",
    "\n",
    "idx = np.random.choice(N*T, N_train, replace=False)\n",
    "x_train = torch.Tensor(x[idx,:])\n",
    "y_train = torch.Tensor(y[idx,:])\n",
    "t_train = torch.Tensor(t[idx,:])\n",
    "u_train = torch.Tensor(u[idx,:])\n",
    "v_train = torch.Tensor(v[idx,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[8.0000],\n        [3.5455],\n        [1.8485],\n        ...,\n        [6.6566],\n        [2.6970],\n        [2.4141]])"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinn import PINN\n",
    "\n",
    "N_train = 5000\n",
    "\n",
    "model   = PINN(x_train, \n",
    "               y_train, \n",
    "               t_train, \n",
    "               u_train,\n",
    "               v_train,\n",
    "               layers_size= [3, 5],    # indentify from where this 3 comes from  \n",
    "               out_size = 2,     # psi and p\n",
    "               params_list= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[Parameter containing:\ntensor([0.], requires_grad=True), Parameter containing:\ntensor([0.], requires_grad=True), Parameter containing:\ntensor([[ 0.8225,  0.8258, -1.2131],\n        [ 0.3490, -0.0853, -0.2740],\n        [-0.3159, -0.3708, -0.0325],\n        [-1.9583,  0.0372, -0.5937],\n        [ 0.6488, -0.5891,  0.6909]], requires_grad=True), Parameter containing:\ntensor([0., 0., 0., 0., 0.], requires_grad=True), Parameter containing:\ntensor([[ 0.0088, -0.8376, -0.0334, -0.5507, -0.9448],\n        [-0.3521, -1.0611,  0.4877, -0.0549, -0.4529]], requires_grad=True), Parameter containing:\ntensor([0., 0.], requires_grad=True)]\n"
    }
   ],
   "source": [
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e9e563401182>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mu_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mloss_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mloss_print\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/DOC/PINNs/pinn.py\u001b[0m in \u001b[0;36mnet\u001b[0;34m(self, x, y, t)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mpsi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsi_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsi_p\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;31m### Gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mgrad_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0mgrad_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(params= model.parameters(), \n",
    "                      lr= 0.001, \n",
    "                      weight_decay= 0.0)\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    u_hat, v_hat, p_hat, f_u, f_v = model.net(x_train, y_train, t_train)\n",
    "    loss_ = model.loss(u_train, v_train, u_hat, v_hat, f_u, f_v)\n",
    "    loss_print  = loss_\n",
    "\n",
    "    optimizer.zero_grad()   # Clear gradients for the next mini-batches\n",
    "    loss_.backward()         # Backpropagation, compute gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "\n",
    "    ### Training status\n",
    "    print('Epoch %d, Loss= %.10f, Time= %.4f' % (epoch, \n",
    "                                                loss_print,\n",
    "                                                t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([5000, 3])\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[ 8.0000,  1.0204, 11.5000],\n        [ 3.5455, -1.0204, 12.8000],\n        [ 1.8485,  1.1837,  0.1000],\n        ...,\n        [ 6.6566, -0.2857, 16.5000],\n        [ 2.6970, -1.5102,  1.0000],\n        [ 2.4141,  0.2041,  0.5000]])"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "X = torch.cat([x_train, y_train, t_train], dim=1)\n",
    "print(X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "PINN(\n  (layers): ModuleList(\n    (0): Linear(in_features=3, out_features=5, bias=True)\n  )\n  (out): Linear(in_features=5, out_features=2, bias=True)\n)"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[-0.0192, -0.0883],\n        [ 0.4411,  0.5885],\n        [-0.3013, -1.4059],\n        ...,\n        [ 0.4463,  0.5065],\n        [-0.9377, -1.1685],\n        [-0.8117, -1.6665]], grad_fn=<AddmmBackward>)"
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "psi_p = model.forward(X)\n",
    "psi_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tensor([[-0.0192],\n        [ 0.4411],\n        [-0.3013],\n        ...,\n        [ 0.4463],\n        [-0.9377],\n        [-0.8117]], grad_fn=<SliceBackward>)\n"
    }
   ],
   "source": [
    "psi = psi_p[:, 0:1]\n",
    "p = psi_p[:, 1:2]\n",
    "print(psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[ 1.0204],\n        [-1.0204],\n        [ 1.1837],\n        ...,\n        [-0.2857],\n        [-1.5102],\n        [ 0.2041]])"
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_np = psi.detach().numpy()\n",
    "y_train_np = y_train.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[-0.01918721],\n       [ 0.4410771 ],\n       [-0.3013378 ],\n       ...,\n       [ 0.44626236],\n       [-0.9376714 ],\n       [-0.8117029 ]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 72
    }
   ],
   "source": [
    "psi_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "None\n"
    }
   ],
   "source": [
    "out = tf.gradients(psi_np, y_train_np)[0]\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch.Size([5000])\ntorch.Size([5000])\n"
    }
   ],
   "source": [
    "print(psi.shape)\n",
    "print(y_train_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor 'concat_1:0' shape=(5000, 3) dtype=float32>"
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "tf.concat([x_train, y_train, t_train], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-fbafc5fced9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpsi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "psi.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([-0.0192], grad_fn=<SelectBackward>)"
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "psi[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-e6e2adc342a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsi_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    147\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         inputs, allow_unused)\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior."
     ]
    }
   ],
   "source": [
    "torch.autograd.grad(psi_[0], y_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[-2.],\n       [-2.],\n       [-2.],\n       ...,\n       [ 2.],\n       [ 2.],\n       [ 2.]])"
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-fbafc5fced9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpsi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "psi.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([233.])"
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "x = torch.autograd.Variable(torch.Tensor([2]),requires_grad=True)\n",
    "y = 5*x**4 + 3*x**3 + 7*x**2 + 9*x - 5\n",
    "\n",
    "y.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([145.], grad_fn=<SubBackward0>)"
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([-0.0192,  0.4411, -0.3013,  ...,  0.4463, -0.9377, -0.8117],\n       grad_fn=<SelectBackward>)"
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-93d030c518bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpsi_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpsi_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "y_train_ = torch.autograd.Variable(torch.Tensor(y_train),requires_grad=True)\n",
    "psi_ = psi.clone()\n",
    "\n",
    "psi_.backward()\n",
    "y_train.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([-0.0192,  0.4411, -0.3013,  ...,  0.4463, -0.9377, -0.8117],\n       grad_fn=<SelectBackward>)"
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[tensor(1., grad_fn=<MulBackward0>), tensor(4., grad_fn=<MulBackward0>), tensor(9., grad_fn=<MulBackward0>), tensor(16., grad_fn=<MulBackward0>), tensor(25., grad_fn=<MulBackward0>)]\ntensor([2., 0., 0., 0., 0.])\n"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def basic_fun(x_cloned):\n",
    "    res = []\n",
    "    for i in range(len(x)):\n",
    "        res.append(x_cloned[i] * x_cloned[i])\n",
    "    print(res)\n",
    "    #return Variable(torch.FloatTensor(res))\n",
    "    return res[0]\n",
    "\n",
    "def get_grad(inp, grad_var):\n",
    "    A = basic_fun(inp)\n",
    "    A.backward()\n",
    "    return grad_var.grad\n",
    "\n",
    "\n",
    "x = nn.Parameter(torch.FloatTensor([1, 2, 3, 4, 5]), requires_grad=True)\n",
    "x_cloned = x.clone()\n",
    "print(get_grad(x_cloned, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([1., 2., 3., 4., 5.], grad_fn=<CloneBackward>)"
     },
     "metadata": {},
     "execution_count": 95
    }
   ],
   "source": [
    "x_cloned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Parameter containing:\ntensor([1., 2., 3., 4., 5.], requires_grad=True)"
     },
     "metadata": {},
     "execution_count": 96
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[-0.0192],\n        [ 0.4411],\n        [-0.3013],\n        ...,\n        [ 0.4463],\n        [-0.9377],\n        [-0.8117]], grad_fn=<SliceBackward>)"
     },
     "metadata": {},
     "execution_count": 97
    }
   ],
   "source": [
    "psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[-0.0192],\n        [ 0.4411],\n        [-0.3013],\n        ...,\n        [ 0.4463],\n        [-0.9377],\n        [-0.8117]], grad_fn=<SliceBackward>)"
     },
     "metadata": {},
     "execution_count": 98
    }
   ],
   "source": [
    "psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "-0.019187211990356445"
     },
     "metadata": {},
     "execution_count": 103
    }
   ],
   "source": [
    "psi[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-c9be416a0657>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mgrad_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     \u001b[0mgrad_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "torch.autograd.grad(psi, x_train ,create_graph=True,retain_graph=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-fbd307eac0e2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpsi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "psi[0].backward(create_graph=True,retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from copy import deepcopy\n",
    "\n",
    "def get_gradient(f, x):\n",
    "    \"\"\" computes gradient of tensor f with respect to tensor x \"\"\"\n",
    "    assert x.requires_grad\n",
    "\n",
    "    x_shape = x.shape\n",
    "    f_shape = f.shape\n",
    "    f = f.view(-1)\n",
    "\n",
    "    x_grads = []\n",
    "    for f_val in f:\n",
    "        if x.grad is not None:\n",
    "            x.grad.data.zero_()\n",
    "        f_val.backward(retain_graph=True)\n",
    "        if x.grad is not None:\n",
    "            x_grads.append(deepcopy(x.grad.data))\n",
    "        else:\n",
    "            # in case f isn't a function of x\n",
    "            x_grads.append(torch.zeros(x.shape).to(x))\n",
    "    output_shape = list(f_shape) + list(x_shape)\n",
    "    return torch.cat((x_grads)).view(output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[ 1.0204],\n        [-1.0204],\n        [ 1.1837],\n        ...,\n        [-0.2857],\n        [-1.5102],\n        [ 0.2041]])"
     },
     "metadata": {},
     "execution_count": 112
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[-0.0192],\n        [ 0.4411],\n        [-0.3013],\n        ...,\n        [ 0.4463],\n        [-0.9377],\n        [-0.8117]], grad_fn=<SliceBackward>)"
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "source": [
    "psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-9f7e906c051e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsi_cp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_cp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-111-4f30d8eefc34>\u001b[0m in \u001b[0;36mget_gradient\u001b[0;34m(f, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mf_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mx_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "get_gradient(psi_cp, y_train_cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[ 1.0204],\n        [-1.0204],\n        [ 1.1837],\n        ...,\n        [-0.2857],\n        [-1.5102],\n        [ 0.2041]], requires_grad=True)"
     },
     "metadata": {},
     "execution_count": 119
    }
   ],
   "source": [
    "y_train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_cp = y_train_.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_cp = psi_.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}