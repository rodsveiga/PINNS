{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596724044761",
   "display_name": "Python 3.8.3 64-bit ('updated': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINNs: a start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import scipy.io\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from pinn import PINN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navier-Stokes equation\n",
    "\n",
    "Given a two-dimensional velocity field, \n",
    "\n",
    "$$ {\\bf V}(x, y; t)  =  ( u(x, y; t), v(x, y; t) )  \\; \\;,  $$\n",
    "its components satisfy the following equations\n",
    "\n",
    "$$ u_t + \\lambda_1 ( u u_x + v u_y ) = -p_x + \\lambda_2 ( u_{xx} + u_{yy}) $$\n",
    "$$ v_t + \\lambda_1 ( u v_x + v v_y ) = -p_y+ \\lambda_2 ( v_{xx} + v_{yy}) $$\n",
    "with $ p = p(x, y; t)$ being the pressure. The parameters $\\lambda_1 $ and $\\lambda_2$ are unknown.\n",
    "\n",
    "We are interested in learning the parameters $\\{ \\lambda \\} $ as well as the pressure $ p(x, y; t)$.\n",
    "\n",
    "Solutions are searched in a set satisfying continuity equation $ \\nabla \\cdot {\\bf V}(x, y; t) = 0 $, \n",
    "\n",
    "$$ u_x + v_y = 0  \\; \\;.$$\n",
    "\n",
    "Defining a latent function $ \\psi = \\psi(x, y; t) $ such that (how crucial is the use of $\\psi$?):\n",
    "$$ u = \\psi_y   \\;, $$ \n",
    "$$ v = - \\psi_x  \\;, $$ \n",
    "the continuity equation is satistied. Given a set ${\\cal S}$ of (noisy) measurements of the velocity field, \n",
    "\n",
    "$$    {\\cal S} = \\{ t^{(j)}, x^{(j)}, y^{(j)} , u^{(j)}, v^{(j)}   \\}_{j=1}^{N}  \\;, $$\n",
    "we define\n",
    "\n",
    "$$ f(x, y; t) \\equiv u_t + \\lambda_1 ( u u_x + v u_y )  + p_x - \\lambda_2 ( u_{xx} + u_{yy}) \\;,$$    \n",
    "$$  g(x, y; t) \\equiv v_t + \\lambda_1 ( u v_x + v v_y ) + p_y - \\lambda_2 ( v_{xx} + v_{yy})  \\;,$$\n",
    "and proceed by jointly approximating\n",
    "\n",
    "$$ \\left[   \\psi(x, y; t) ;  p(x, y; t)  \\right]  \\;, $$\n",
    "using a single neural network with two outputs.\n",
    "\n",
    "The prior assumption is taking into account in another neural network (PINN) with two outputs:\n",
    "\n",
    "$$ \\left[   f(x, y; t) ;  g(x, y; t)  \\right]  \\;. $$\n",
    "\n",
    "The parameters $\\{ \\lambda \\} $ operate as well as the parameters of the neural networks:\n",
    "\n",
    "\n",
    "$$ \\left[   \\psi(x, y; t) ;  p(x, y; t)  \\right]  \\;, $$\n",
    "$$ \\left[   f(x, y; t) ;  g(x, y; t)  \\right]  \\;, $$\n",
    "that can be trained using a mean squared error loss:\n",
    "\n",
    "$$  MSE \\equiv \\frac{1}{N} \\sum_{j=1}^{N} \\left[  \\left( u( x^{(j)}, y^{(j)}, t^{(j)}) - u^{(j)} \\right)^2 + \\left( v( x^{(j)}, y^{(j)}, t^{(j)}) - v^{(j)} \\right)^2  \\right]  + \\frac{1}{N} \\sum_{j=1}^{N} \\left[   |  u( x^{(j)}, y^{(j)}, t^{(j)}) |^2 +  |g( x^{(j)}, y^{(j)}, t^{(j)})|^2       \\right]     $$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "From https://github.com/maziarraissi/PINNs/blob/master/main/Data/cylinder_nektar_wake.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('data/cylinder_nektar_wake.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(200, 1)\n(5000, 2)\n(5000, 2, 200)\n(5000, 200)\n"
    }
   ],
   "source": [
    "print(data['t'].shape)\n",
    "print(data['X_star'].shape)\n",
    "print(data['U_star'].shape)\n",
    "print(data['p_star'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_star = data['U_star'] # N x 2 x T\n",
    "p_star = data['p_star'] # N x T\n",
    "t_star = data['t'] # T x 1\n",
    "X_star = data['X_star'] # N x 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X_star.shape[0]\n",
    "T = t_star.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange Data \n",
    "XX = np.tile(X_star[:, 0:1], (1, T)) # N x T\n",
    "YY = np.tile(X_star[:, 1:2], (1, T)) # N x T\n",
    "TT = np.tile(t_star, (1, N)).T # N x T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(5000, 200)\n(5000, 200)\n(5000, 200)\n"
    }
   ],
   "source": [
    "print(XX.shape)\n",
    "print(YY.shape)\n",
    "print(TT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange Data \n",
    "UU = U_star[:, 0, :] # N x T\n",
    "VV = U_star[:, 1, :] # N x T\n",
    "pp = p_star # N x T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(5000, 200)\n(5000, 200)\n(5000, 200)\n"
    }
   ],
   "source": [
    "print(UU.shape)\n",
    "print(VV.shape)\n",
    "print(pp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Flattening\n",
    "x = XX.flatten()[:, None] # NT x 1\n",
    "y = YY.flatten()[:, None] # NT x 1\n",
    "t = TT.flatten()[:, None] # NT x 1\n",
    "    \n",
    "u = UU.flatten()[:, None] # NT x 1\n",
    "v = VV.flatten()[:, None] # NT x 1\n",
    "p = pp.flatten()[:, None] # NT x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1000000, 1)\n(1000000, 1)\n(1000000, 1)\n(1000000, 1)\n(1000000, 1)\n(1000000, 1)\n"
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(t.shape)\n",
    "print(u.shape)\n",
    "print(v.shape)\n",
    "print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data    \n",
    "N_train = 100\n",
    "\n",
    "idx = np.random.choice(N*T, N_train, replace=False)\n",
    "x_train = torch.Tensor(x[idx,:])\n",
    "x_train = x_train.reshape(x_train.shape[0])\n",
    "\n",
    "y_train = torch.Tensor(y[idx,:])\n",
    "y_train = y_train.reshape(y_train.shape[0])\n",
    "\n",
    "t_train = torch.Tensor(t[idx,:])\n",
    "t_train = t_train.reshape(t_train.shape[0])\n",
    "\n",
    "u_train = torch.Tensor(u[idx,:])\n",
    "u_train = u_train.reshape(u_train.shape[0])\n",
    "\n",
    "v_train = torch.Tensor(v[idx,:])\n",
    "v_train = v_train.reshape(v_train.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 0, Loss= 0.9530798793, lambda_1= -0.1128, lambda_2= -0.0542 Time= 14.30\nEpoch 1, Loss= 0.9537823200, lambda_1= -0.0187, lambda_2= 0.0337 Time= 15.10\nEpoch 2, Loss= 0.9534051418, lambda_1= 0.0587, lambda_2= 0.0873 Time= 14.90\nEpoch 3, Loss= 0.9533430934, lambda_1= 0.1066, lambda_2= 0.0993 Time= 15.02\nEpoch 4, Loss= 0.9533370733, lambda_1= 0.1223, lambda_2= 0.0827 Time= 15.09\nEpoch 5, Loss= 0.9533377886, lambda_1= 0.1130, lambda_2= 0.0505 Time= 14.97\nEpoch 6, Loss= 0.9533371329, lambda_1= 0.0874, lambda_2= 0.0120 Time= 15.05\nEpoch 7, Loss= 0.9533358216, lambda_1= 0.0525, lambda_2= -0.0243 Time= 15.30\nEpoch 8, Loss= 0.9533355236, lambda_1= 0.0141, lambda_2= -0.0514 Time= 15.09\nEpoch 9, Loss= 0.9533349872, lambda_1= -0.0221, lambda_2= -0.0646 Time= 15.24\nEpoch 10, Loss= 0.9533355832, lambda_1= -0.0513, lambda_2= -0.0635 Time= 14.98\nEpoch 11, Loss= 0.9533362985, lambda_1= -0.0700, lambda_2= -0.0507 Time= 15.01\nEpoch 12, Loss= 0.9533358812, lambda_1= -0.0766, lambda_2= -0.0299 Time= 15.22\nEpoch 13, Loss= 0.9533361197, lambda_1= -0.0719, lambda_2= -0.0058 Time= 14.85\nEpoch 14, Loss= 0.9533364773, lambda_1= -0.0581, lambda_2= 0.0171 Time= 15.12\nEpoch 15, Loss= 0.9533362985, lambda_1= -0.0379, lambda_2= 0.0346 Time= 14.96\nEpoch 16, Loss= 0.9533358812, lambda_1= -0.0146, lambda_2= 0.0439 Time= 14.70\nEpoch 17, Loss= 0.9533360004, lambda_1= 0.0086, lambda_2= 0.0442 Time= 14.98\nEpoch 18, Loss= 0.9533362985, lambda_1= 0.0285, lambda_2= 0.0363 Time= 14.88\nEpoch 19, Loss= 0.9533362985, lambda_1= 0.0425, lambda_2= 0.0225 Time= 15.67\nEpoch 20, Loss= 0.9533362985, lambda_1= 0.0493, lambda_2= 0.0057 Time= 15.64\nEpoch 21, Loss= 0.9533362985, lambda_1= 0.0486, lambda_2= -0.0105 Time= 14.93\nEpoch 22, Loss= 0.9533362985, lambda_1= 0.0412, lambda_2= -0.0234 Time= 15.24\nEpoch 23, Loss= 0.9533362985, lambda_1= 0.0287, lambda_2= -0.0305 Time= 14.98\nEpoch 24, Loss= 0.9533362985, lambda_1= 0.0133, lambda_2= -0.0311 Time= 15.13\nEpoch 25, Loss= 0.9533362985, lambda_1= -0.0027, lambda_2= -0.0256 Time= 15.07\nEpoch 26, Loss= 0.9533362985, lambda_1= -0.0168, lambda_2= -0.0156 Time= 14.66\nEpoch 27, Loss= 0.9533362985, lambda_1= -0.0272, lambda_2= -0.0035 Time= 14.93\nEpoch 28, Loss= 0.9533362985, lambda_1= -0.0326, lambda_2= 0.0083 Time= 14.82\nEpoch 29, Loss= 0.9533362985, lambda_1= -0.0327, lambda_2= 0.0174 Time= 15.18\nEpoch 30, Loss= 0.9533362985, lambda_1= -0.0278, lambda_2= 0.0220 Time= 14.94\nEpoch 31, Loss= 0.9533362985, lambda_1= -0.0191, lambda_2= 0.0217 Time= 15.03\nEpoch 32, Loss= 0.9533362985, lambda_1= -0.0083, lambda_2= 0.0169 Time= 15.32\nEpoch 33, Loss= 0.9533362985, lambda_1= 0.0028, lambda_2= 0.0090 Time= 15.02\nEpoch 34, Loss= 0.9533362985, lambda_1= 0.0126, lambda_2= -0.0001 Time= 15.43\nEpoch 35, Loss= 0.9533362985, lambda_1= 0.0194, lambda_2= -0.0084 Time= 15.20\nEpoch 36, Loss= 0.9533362985, lambda_1= 0.0225, lambda_2= -0.0141 Time= 14.91\nEpoch 37, Loss= 0.9533362985, lambda_1= 0.0217, lambda_2= -0.0162 Time= 15.28\nEpoch 38, Loss= 0.9533362985, lambda_1= 0.0173, lambda_2= -0.0145 Time= 15.01\nEpoch 39, Loss= 0.9533362985, lambda_1= 0.0105, lambda_2= -0.0097 Time= 14.86\nEpoch 40, Loss= 0.9533362985, lambda_1= 0.0026, lambda_2= -0.0031 Time= 14.86\nEpoch 41, Loss= 0.9533362985, lambda_1= -0.0051, lambda_2= 0.0036 Time= 14.56\nEpoch 42, Loss= 0.9533362985, lambda_1= -0.0112, lambda_2= 0.0088 Time= 15.09\nEpoch 43, Loss= 0.9533362985, lambda_1= -0.0148, lambda_2= 0.0116 Time= 15.98\nEpoch 44, Loss= 0.9533362985, lambda_1= -0.0155, lambda_2= 0.0113 Time= 15.37\nEpoch 45, Loss= 0.9533362985, lambda_1= -0.0134, lambda_2= 0.0084 Time= 15.15\nEpoch 46, Loss= 0.9533362985, lambda_1= -0.0091, lambda_2= 0.0037 Time= 15.18\nEpoch 47, Loss= 0.9533362985, lambda_1= -0.0036, lambda_2= -0.0015 Time= 14.99\nEpoch 48, Loss= 0.9533362985, lambda_1= 0.0021, lambda_2= -0.0058 Time= 14.84\nEpoch 49, Loss= 0.9533362985, lambda_1= 0.0069, lambda_2= -0.0083 Time= 14.65\nEpoch 50, Loss= 0.9533362985, lambda_1= 0.0100, lambda_2= -0.0085 Time= 14.71\nEpoch 51, Loss= 0.9533362985, lambda_1= 0.0109, lambda_2= -0.0065 Time= 14.62\nEpoch 52, Loss= 0.9533362985, lambda_1= 0.0097, lambda_2= -0.0031 Time= 15.30\nEpoch 53, Loss= 0.9533362985, lambda_1= 0.0067, lambda_2= 0.0008 Time= 16.16\nEpoch 54, Loss= 0.9533362985, lambda_1= 0.0028, lambda_2= 0.0042 Time= 15.03\nEpoch 55, Loss= 0.9533362985, lambda_1= -0.0014, lambda_2= 0.0061 Time= 15.90\nEpoch 56, Loss= 0.9533362985, lambda_1= -0.0048, lambda_2= 0.0063 Time= 15.07\nEpoch 57, Loss= 0.9533362985, lambda_1= -0.0071, lambda_2= 0.0047 Time= 14.97\nEpoch 58, Loss= 0.9533362985, lambda_1= -0.0077, lambda_2= 0.0020 Time= 15.25\nEpoch 59, Loss= 0.9533362985, lambda_1= -0.0067, lambda_2= -0.0009 Time= 15.03\nEpoch 60, Loss= 0.9533362985, lambda_1= -0.0044, lambda_2= -0.0034 Time= 15.26\nEpoch 61, Loss= 0.9533362985, lambda_1= -0.0015, lambda_2= -0.0047 Time= 14.90\nEpoch 62, Loss= 0.9533362985, lambda_1= 0.0015, lambda_2= -0.0045 Time= 15.26\nEpoch 63, Loss= 0.9533362985, lambda_1= 0.0039, lambda_2= -0.0032 Time= 15.89\nEpoch 64, Loss= 0.9533362985, lambda_1= 0.0052, lambda_2= -0.0010 Time= 15.58\nEpoch 65, Loss= 0.9533362985, lambda_1= 0.0054, lambda_2= 0.0012 Time= 15.47\nEpoch 66, Loss= 0.9533362985, lambda_1= 0.0044, lambda_2= 0.0029 Time= 15.04\nEpoch 67, Loss= 0.9533362985, lambda_1= 0.0025, lambda_2= 0.0036 Time= 15.16\nEpoch 68, Loss= 0.9533362985, lambda_1= 0.0003, lambda_2= 0.0032 Time= 15.40\nEpoch 69, Loss= 0.9533362985, lambda_1= -0.0018, lambda_2= 0.0018 Time= 15.10\nEpoch 70, Loss= 0.9533362985, lambda_1= -0.0033, lambda_2= 0.0001 Time= 15.09\nEpoch 71, Loss= 0.9533362985, lambda_1= -0.0039, lambda_2= -0.0015 Time= 14.97\nEpoch 72, Loss= 0.9533362985, lambda_1= -0.0036, lambda_2= -0.0025 Time= 15.10\nEpoch 73, Loss= 0.9533362985, lambda_1= -0.0025, lambda_2= -0.0027 Time= 15.21\nEpoch 74, Loss= 0.9533362985, lambda_1= -0.0010, lambda_2= -0.0020 Time= 15.10\nEpoch 75, Loss= 0.9533362985, lambda_1= 0.0006, lambda_2= -0.0008 Time= 15.08\nEpoch 76, Loss= 0.9533362985, lambda_1= 0.0019, lambda_2= 0.0005 Time= 15.25\nEpoch 77, Loss= 0.9533362985, lambda_1= 0.0027, lambda_2= 0.0016 Time= 14.98\nEpoch 78, Loss= 0.9533362985, lambda_1= 0.0028, lambda_2= 0.0020 Time= 15.13\nEpoch 79, Loss= 0.9533362985, lambda_1= 0.0022, lambda_2= 0.0018 Time= 15.22\nEpoch 80, Loss= 0.9533362985, lambda_1= 0.0011, lambda_2= 0.0010 Time= 15.37\nEpoch 81, Loss= 0.9533362985, lambda_1= -0.0001, lambda_2= -0.0000 Time= 15.10\nEpoch 82, Loss= 0.9533362985, lambda_1= -0.0011, lambda_2= -0.0009 Time= 15.24\nEpoch 83, Loss= 0.9533362985, lambda_1= -0.0018, lambda_2= -0.0015 Time= 15.19\nEpoch 84, Loss= 0.9533362985, lambda_1= -0.0020, lambda_2= -0.0015 Time= 14.83\nEpoch 85, Loss= 0.9533362985, lambda_1= -0.0017, lambda_2= -0.0010 Time= 15.38\nEpoch 86, Loss= 0.9533362985, lambda_1= -0.0010, lambda_2= -0.0003 Time= 15.18\nEpoch 87, Loss= 0.9533362985, lambda_1= -0.0001, lambda_2= 0.0005 Time= 15.26\nEpoch 88, Loss= 0.9533362985, lambda_1= 0.0007, lambda_2= 0.0011 Time= 15.25\nEpoch 89, Loss= 0.9533362985, lambda_1= 0.0013, lambda_2= 0.0012 Time= 15.08\nEpoch 90, Loss= 0.9533362985, lambda_1= 0.0015, lambda_2= 0.0009 Time= 15.38\nEpoch 91, Loss= 0.9533362985, lambda_1= 0.0013, lambda_2= 0.0003 Time= 15.20\nEpoch 92, Loss= 0.9533362985, lambda_1= 0.0008, lambda_2= -0.0003 Time= 15.08\nEpoch 93, Loss= 0.9533362985, lambda_1= 0.0002, lambda_2= -0.0007 Time= 14.86\nEpoch 94, Loss= 0.9533362985, lambda_1= -0.0005, lambda_2= -0.0009 Time= 15.30\nEpoch 95, Loss= 0.9533362985, lambda_1= -0.0009, lambda_2= -0.0007 Time= 15.69\nEpoch 96, Loss= 0.9533362985, lambda_1= -0.0011, lambda_2= -0.0003 Time= 15.16\nEpoch 97, Loss= 0.9533362985, lambda_1= -0.0010, lambda_2= 0.0001 Time= 14.73\nEpoch 98, Loss= 0.9533362985, lambda_1= -0.0006, lambda_2= 0.0005 Time= 15.20\nEpoch 99, Loss= 0.9533362985, lambda_1= -0.0001, lambda_2= 0.0007 Time= 14.91\n"
    }
   ],
   "source": [
    "layers = [3, 20, 20, 20, 20, 20, 20, 20, 20]\n",
    "#layers = [3, 20, 20]\n",
    "\n",
    "model   = PINN(x_train, \n",
    "               y_train, \n",
    "               t_train, \n",
    "               u_train,\n",
    "               v_train,\n",
    "               layers_size= layers,    # indentify from where this 3 comes from  \n",
    "               out_size = 2,     # psi and p\n",
    "               params_list= None)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(params= model.parameters(), \n",
    "                      lr= 0.1, \n",
    "                      weight_decay= 0.01)\n",
    "\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    u_hat, v_hat, p_hat, f_u, f_v = model.net(x_train, y_train, t_train)\n",
    "    \n",
    "    loss_ = model.loss(u_train, v_train, u_hat, v_hat, f_u, f_v)\n",
    "    loss_print  = loss_\n",
    "\n",
    "    optimizer.zero_grad()   # Clear gradients for the next mini-batches\n",
    "    loss_.backward()         # Backpropagation, compute gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "\n",
    "    ### Training status\n",
    "    print('Epoch %d, Loss= %.10f, lambda_1= %.4f, lambda_2= %.4f Time= %.2f' % (epoch, \n",
    "                                                                                loss_print,\n",
    "                                                                                list(model.parameters())[0].item(),\n",
    "                                                                                list(model.parameters())[1].item(),\n",
    "                                                                                t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}