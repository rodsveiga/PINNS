{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1596718745526",
   "display_name": "Python 3.8.3 64-bit ('updated': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINNs: a start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import scipy.io\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from pinn import PINN\n",
    "from net import Net\n",
    "#from ns import PhysicsInformedNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navier-Stokes equation\n",
    "\n",
    "Given a two-dimensional velocity field, \n",
    "\n",
    "$$ {\\bf V}(x, y; t)  =  ( u(x, y; t), v(x, y; t) )  \\; \\;,  $$\n",
    "its components satisfy the following equations\n",
    "\n",
    "$$ u_t + \\lambda_1 ( u u_x + v u_y ) = -p_x + \\lambda_2 ( u_{xx} + u_{yy}) $$\n",
    "$$ v_t + \\lambda_1 ( u v_x + v v_y ) = -p_y+ \\lambda_2 ( v_{xx} + v_{yy}) $$\n",
    "with $ p = p(x, y; t)$ being the pressure. The parameters $\\lambda_1 $ and $\\lambda_2$ are unknown.\n",
    "\n",
    "We are interested in learning the parameters $\\{ \\lambda \\} $ as well as the pressure $ p(x, y; t)$.\n",
    "\n",
    "Solutions are searched in a set satisfying continuity equation $ \\nabla \\cdot {\\bf V}(x, y; t) = 0 $, \n",
    "\n",
    "$$ u_x + v_y = 0  \\; \\;.$$\n",
    "\n",
    "Defining a latent function $ \\psi = \\psi(x, y; t) $ such that (how crucial is the use of $\\psi$?):\n",
    "$$ u = \\psi_y   \\;, $$ \n",
    "$$ v = - \\psi_x  \\;, $$ \n",
    "the continuity equation is satistied. Given a set ${\\cal S}$ of (noisy) measurements of the velocity field, \n",
    "\n",
    "$$    {\\cal S} = \\{ t^{(j)}, x^{(j)}, y^{(j)} , u^{(j)}, v^{(j)}   \\}_{j=1}^{N}  \\;, $$\n",
    "we define\n",
    "\n",
    "$$ f(x, y; t) \\equiv u_t + \\lambda_1 ( u u_x + v u_y )  + p_x - \\lambda_2 ( u_{xx} + u_{yy}) \\;,$$    \n",
    "$$  g(x, y; t) \\equiv v_t + \\lambda_1 ( u v_x + v v_y ) + p_y - \\lambda_2 ( v_{xx} + v_{yy})  \\;,$$\n",
    "and proceed by jointly approximating\n",
    "\n",
    "$$ \\left[   \\psi(x, y; t) ;  p(x, y; t)  \\right]  \\;, $$\n",
    "using a single neural network with two outputs.\n",
    "\n",
    "The prior assumption is taking into account in another neural network (PINN) with two outputs:\n",
    "\n",
    "$$ \\left[   f(x, y; t) ;  g(x, y; t)  \\right]  \\;. $$\n",
    "\n",
    "The parameters $\\{ \\lambda \\} $ operate as well as the parameters of the neural networks:\n",
    "\n",
    "\n",
    "$$ \\left[   \\psi(x, y; t) ;  p(x, y; t)  \\right]  \\;, $$\n",
    "$$ \\left[   f(x, y; t) ;  g(x, y; t)  \\right]  \\;, $$\n",
    "that can be trained using a mean squared error loss:\n",
    "\n",
    "$$  MSE \\equiv \\frac{1}{N} \\sum_{j=1}^{N} \\left[  \\left( u( x^{(j)}, y^{(j)}, t^{(j)}) - u^{(j)} \\right)^2 + \\left( v( x^{(j)}, y^{(j)}, t^{(j)}) - v^{(j)} \\right)^2  \\right]  + \\frac{1}{N} \\sum_{j=1}^{N} \\left[   |  u( x^{(j)}, y^{(j)}, t^{(j)}) |^2 +  |g( x^{(j)}, y^{(j)}, t^{(j)})|^2       \\right]     $$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "From https://github.com/maziarraissi/PINNs/blob/master/main/Data/cylinder_nektar_wake.mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('data/cylinder_nektar_wake.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(200, 1)\n(5000, 2)\n(5000, 2, 200)\n(5000, 200)\n"
    }
   ],
   "source": [
    "print(data['t'].shape)\n",
    "print(data['X_star'].shape)\n",
    "print(data['U_star'].shape)\n",
    "print(data['p_star'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_star = data['U_star'] # N x 2 x T\n",
    "p_star = data['p_star'] # N x T\n",
    "t_star = data['t'] # T x 1\n",
    "X_star = data['X_star'] # N x 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X_star.shape[0]\n",
    "T = t_star.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange Data \n",
    "XX = np.tile(X_star[:, 0:1], (1, T)) # N x T\n",
    "YY = np.tile(X_star[:, 1:2], (1, T)) # N x T\n",
    "TT = np.tile(t_star, (1, N)).T # N x T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(5000, 200)\n(5000, 200)\n(5000, 200)\n"
    }
   ],
   "source": [
    "print(XX.shape)\n",
    "print(YY.shape)\n",
    "print(TT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rearrange Data \n",
    "UU = U_star[:, 0, :] # N x T\n",
    "VV = U_star[:, 1, :] # N x T\n",
    "pp = p_star # N x T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(5000, 200)\n(5000, 200)\n(5000, 200)\n"
    }
   ],
   "source": [
    "print(UU.shape)\n",
    "print(VV.shape)\n",
    "print(pp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Flattening\n",
    "x = XX.flatten()[:, None] # NT x 1\n",
    "y = YY.flatten()[:, None] # NT x 1\n",
    "t = TT.flatten()[:, None] # NT x 1\n",
    "    \n",
    "u = UU.flatten()[:, None] # NT x 1\n",
    "v = VV.flatten()[:, None] # NT x 1\n",
    "p = pp.flatten()[:, None] # NT x 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1000000, 1)\n(1000000, 1)\n(1000000, 1)\n(1000000, 1)\n(1000000, 1)\n(1000000, 1)\n"
    }
   ],
   "source": [
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(t.shape)\n",
    "print(u.shape)\n",
    "print(v.shape)\n",
    "print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data    \n",
    "N_train = 100\n",
    "\n",
    "idx = np.random.choice(N*T, N_train, replace=False)\n",
    "x_train = torch.Tensor(x[idx,:])\n",
    "y_train = torch.Tensor(y[idx,:])\n",
    "t_train = torch.Tensor(t[idx,:])\n",
    "u_train = torch.Tensor(u[idx,:])\n",
    "v_train = torch.Tensor(v[idx,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 0, Loss= 0.9246909618, Time= 45.5933\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-41904f8247b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mu_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mloss_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_u\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mloss_print\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mloss_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/DOC/PINNs/pinn.py\u001b[0m in \u001b[0;36mnet\u001b[0;34m(self, x, y, t)\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mv_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0mv_xx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         \u001b[0mv_yy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/DOC/PINNs/pinn.py\u001b[0m in \u001b[0;36mget_gradient\u001b[0;34m(self, f, x)\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m             \u001b[0mf_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0mx_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/updated/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/updated/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layers = [3, 20, 20, 20, 20, 20, 20, 20, 20]\n",
    "#layers = [3, 20, 20]\n",
    "\n",
    "model   = PINN(x_train, \n",
    "               y_train, \n",
    "               t_train, \n",
    "               u_train,\n",
    "               v_train,\n",
    "               layers_size= layers,    # indentify from where this 3 comes from  \n",
    "               out_size = 2,     # psi and p\n",
    "               params_list= None)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(params= model.parameters(), \n",
    "                      lr= 0.1, \n",
    "                      weight_decay= 0.01)\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    u_hat, v_hat, p_hat, f_u, f_v = model.net(x_train, y_train, t_train)\n",
    "    loss_ = model.loss(u_train, v_train, u_hat, v_hat, f_u, f_v)\n",
    "    loss_print  = loss_\n",
    "\n",
    "    optimizer.zero_grad()   # Clear gradients for the next mini-batches\n",
    "    loss_.backward()         # Backpropagation, compute gradients\n",
    "    optimizer.step()\n",
    "\n",
    "    t1 = time.time()\n",
    "\n",
    "\n",
    "    ### Training status\n",
    "    print('Epoch %d, Loss= %.10f, Time= %.4f' % (epoch, \n",
    "                                                loss_print,\n",
    "                                                t1-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}